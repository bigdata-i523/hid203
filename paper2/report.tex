\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Big Data for Edge Computing}


\author{Ben Trovato}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin} 
  \state{Ohio} 
  \postcode{43017-6221}
}
\email{trovato@corporation.com}

\author{G.K.M. Tobin}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin} 
  \state{Ohio} 
  \postcode{43017-6221}
}
\email{webmaster@marysville-ohio.com}

\author{Gregor von Laszewski}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{Smith Research Center}
  \city{Bloomington} 
  \state{IN} 
  \postcode{47408}
  \country{USA}}
\email{laszewski@gmail.com}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}
This paper provides a sample of a \LaTeX\ document which conforms,
somewhat loosely, to the formatting guidelines for
ACM SIG Proceedings.
\end{abstract}

\keywords{Big Data, Spark, Hadoop, R, Predictive Modeling, Regression, i523}


\maketitle



\section{Introduction}

Put here an introduction about your topic. 
We just need one sample refernce so the paper compiles in LaTeX so we
put it here \cite{editor00}.

\section{Big Data and Statistics}

Put here an introduction about your topic. 
We just need one sample refernce so the paper compiles in LaTeX so we
put it here \cite{editor00}.


\section{Traditional Predictive Modeling}

Predictive modeling is one of the most important type of statistical analysis of big data. It aims to model the causal relationship between different features present in the data. Specifically, we try to predict the value of one variable, known as the dependent or response variable, with the help of one or more variables, known as independent or explanatory variables. The traditional predictive modeling process is shown in Figure 1.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/fly.pdf}
  \caption{Example caption}\label{f:fly}
\end{figure}

The steps in traditional predictive modeling are: \cite{part-reg}

\subsection{Define Goal}
For any predictive modeling, it is important to clearly define the goal, i.e., define the response variable and the explanatory variables that we are going to use to predict the response variable.
\subsection{Data Collection and Management} 
This is another critical step for predictive modeling which requires identifying the data that can be used for analysis. It can be the most time consuming step in the entire modeling process and may require some preliminary data exploration and visualization. It also involves identifying the feature set and the structure of each feature.
\subsection{Data Preprocessing} 
Before building any predictive model, it is important to perform data quality checks. Two major components of data preprocessing are 
\begin{itemize}
	\item Analyzing missing values: For missing values, it is important to identify whether we want to drop the missing entries in the data or impute them using standard imputation methods such as mean imputation for quantitative features and mode imputation for qualitative features.
	\item Data transformation: The aim of data transformation is to convert it into a form which is easier to model. For example, normalization and standardization of data helps in better interpretation of the co-efficients of regression model. Some of the transformations also depend on the predictive model that we intend to apply. For example, for a linear regression model, it is important to ensure that the dependent and the independent variables have a linear relationship and that the dependent variable has a constant variance.
\end{itemize}
\subsection{Exploratory Data Analysis (EDA)} 
As part of EDA, we try to summarize the data graphically and analyze each feature along with the relationship between different features. For summarization, a variety of summary statistics such as mean, median, variance, etc. are used. In case of predictive modeling, one might want to explore the data and visualize the numerical summary along with the correlation of different features in the data set.
\subsection{Model-building and reporting}
The final step involves building the predictive model using the clean transformed data. Different regression techniques such as, linear regression, can be used for predicting the response variable from the independent variables. Basically, we try to estimate the co-efficient, $\hat{\beta}$, for each independent variable such that a unit change in the independent variable results in a change of $\hat{\beta}$ units in the mean value of the response variable. Once the model is built, it is evaluated using one of the several metrics like the co-efficient of determination.

\section{Predictive Modeling with Big Data}

After having the background of the traditional predictive modeling, we now study the process of extending this method for big data. Traditional statistical analysis generally rely on a representative sample of data to make inference about the population \cite{part-reg}. However, in case of big data, relying on one sample may lead to incorrect predictions. Thus, we partition big data into subsets such that the size of these subsets is close enough to a sample. We achieve this by using one of the sampling techniques from statistics such as random sampling, stratified sampling or cluster sampling \cite{div-reg}. We then apply regression techniques to each of these samples independently. The final prediction is made by aggregating the results from each of these samples. The architecture for this divide-and-conquer regression analysis is as shown in Figure 2. Except for the model building step, all other steps are the same as that of traditional predictive modeling.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/fly.pdf}
  \caption{Example caption}\label{f:fly}
\end{figure}

This divided regression analysis is summarized as below \cite{div-reg}:

\begin{itemize}
	\item Divide big data: In this step, the data is divided into $M$ subsets by using one of the statistical sampling techniques.
	\item Apply multiple linear regression analysis: Perform regression on each of these subsets and compute their respective regression parameters, $\hat{\beta_1}, \hat{\beta_2}, ...., \hat{\beta_m}$. Combine each of these parameters to compute the final regression co-efficient for the big data, $\hat{\beta_c}$ as below:
	\begin{center}$\hat{\beta_c} = f_c(\hat{\beta_1}, \hat{\beta_2}, ...., \hat{\beta_m})$\end{center}
	Here, $f_c$ is a combine function used for combining the co-efficients obtained from each of the $M$ subsets of data. This function can be defined in various ways based on the data. For example, we can define the combine function as the mean value of all the co-efficients: 
	\begin{center} $\hat{\beta_c} = \frac{1}{M}\sum_{i=1}^M \hat{\beta_i}$ \end{center}
	\item Evaluate the model: As part of model evaluation, we first check the confidence interval for all the co-efficients, obtained by applying the combine function, for all the independent variables. Next, we check the accuracy of the model by using some evaluation metric that measures how accurately the model predicts the value of the response variable. For example, we can use mean squared error ($MSE$) which is calculated as below:
	\begin{center} $MSE = \frac{1}{n}\sum_{i=1}^n (Y_i - \hat{Y_i})^2$ \end{center}
\end{itemize} 


\section{Big Data Systems and the support in R}
As explained in the previous section, regression analysis of big data can be efficiently performed by divide-and-conquer strategy. However, it is important to remember that big data processing deals with large volume of data and thus even in case of divided regression analysis, a single machine may not be enough. Thus, we require a distributed system where subsets of data are processed on multiple machines and the results are later aggregated to produce the final prediction as shown in Figure 3 \cite{dkr-reg}.
\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/fly.pdf}
  \caption{Example caption}\label{f:fly}
\end{figure}
Some of the available frameworks that support such distributed storage and parallel computing include Hadoop and Spark. We can use the power of these frameworks to achieve the distributed version of traditional predictive modeling techniques. Many programming languages support these data intensive computing paradigm. We discuss these frameworks and their support in one of the programming languages, i.e., R \cite{log-reg}. 

\subsection{Hadoop} 
Hadoop is an open-source Java-based distributed computing platform which is used for distributed storage and distributed processing of huge data sets on computer clusters \cite{log-reg}. All the modules of the Hadoop framework are fault-tolerant, i.e., they can automatically handle failures of individual machines in the framework. The four important modules of this framework are \cite{log-reg}:

\begin{itemize}
	\item Hadoop Common provides all the Java libraries, utilities, OS level abstraction and the necessary files required by other modules
	\item Hadoop Distributed File System (HDFS) for storing big data as well as providing high throughput access to this data
	\item MapReduce for processing large volumes of data
	\item Hadoop YARN for resource management and task scheduling
\end{itemize}

R provides RHadoop, a family of R packages, that acts as a wrapper for Hadoop streaming and allows execution of Hadoop jobs.Some of the important packages from RHadoop include rhdfs and rmr. Rhdfs is used for handling the HDFS operations such as file storage and file manipulation whereas rmr is primarily responsible for the map reduce function.

\subsection{Spark} 
Spark is another open-source distributed computing framework, however, unlike Hadoop, Spark is a memory based computing framework. Spark's in-memory processing capabilities often result in better performance as compared to Hadoop, especially when implementing iterative machine learning algorithms. The two key abstractions of Spark are:

\begin{itemize}
	\item Resilient Distributed Datasets: Collection of fault-tolerant data items which can be operated in parallel.
	\item Directed Acyclic Graph (DAG): DAG is a set of vertices and edges where vertices represent the RDDs and edges represent the operations to be applied on the vertices. Spark's DAG engine optimizes the execution by breaking down a Spark job into complex multi-step data pipeline to be executed on the cluster.
\end{itemize}

R provides the package SparkR which is a light-weight frontend for using Apache Spark in R \cite{log-reg}. SparkR provides SparkContext which establishes a connection between the R program and the Spark cluster. Users can then use the RDD class provided by this package to explore the Spark API and interactively trigger jobs from the R shell on to the Spark cluster. SparkR also provides distributed machine learning support through MLlib \cite{log-reg}. Thus, with the help of this package, we can implement distributed version of the regression analysis.

\section{Regression Techniques for Big Data}

Using the distributed version, we can efficiently apply different kinds of regression for big data analysis. We now discuss some of these regression techniques:
\begin{itemize}
	\item Linear Regression
	\item Lasso Regression
	\item Ridge Regression
	\item Logistic Regression
\end{itemize}

\section{Conclusion}

Put here an conclusion. Conlcusions and abstracts must not have any
citations in the section.


\begin{acks}

  We would like to thank Dr. Gregor von Laszewski for his
  support and suggestions to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 


\end{document}
